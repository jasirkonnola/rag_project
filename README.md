# Local PDF RAG with Auto-Summarization ü¶ô

A powerful, privacy-focused application that allows you to chat with your PDF documents locally. Built with Django, LangChain, and Ollama, this tool runs entirely on your machine‚Äîno data leaves your computer.

It features **automatic summarization** upon upload and uses **metadata filtering** to let you query specific documents or your entire library at once.

## ‚ú® Features

* **100% Local Processing:** Uses `Ollama` and open-source embedding models. No API keys required.
* **Auto-Summarization:** Automatically generates a 3-5 bullet point summary for every PDF uploaded.
* **Smart Context Retrieval:** Uses `ChromaDB` to store vectors and retrieve the most relevant 6 chunks of text for high-quality answers.
* **Metadata Filtering:** Chat with a specific PDF or search across "All Documents" simultaneously.
* **Optimized Performance:** configured for `Llama 3.2` (3B) and `all-MiniLM-L6-v2` for fast CPU inference.

## üõ†Ô∏è Tech Stack

* **Backend Framework:** Django
* **LLM Orchestration:** LangChain
* **Local LLM:** Ollama (Llama 3.2)
* **Vector Database:** ChromaDB
* **Embeddings:** HuggingFace (`sentence-transformers/all-MiniLM-L6-v2`)
* **PDF Parsing:** PyMuPDF (Fitz)

---

## üìã Prerequisites

Before running the app, ensure you have the following installed:

1.  **Python 3.10+**
2.  **Ollama:** [Download and install from ollama.com](https://ollama.com/)

### Model Setup
Open your terminal and pull the optimized model (approx. 2GB):

```bash
ollama pull llama3.2

---

üì¶ Installation Guide
1. Clone the Repository
Bash

git clone <your-repo-url>
cd <your-project-folder>
2. Create a Virtual Environment
It is recommended to use a virtual environment to manage dependencies.

Windows:

Bash

python -m venv venv
venv\Scripts\activate
Mac/Linux:

Bash

python3 -m venv venv
source venv/bin/activate
3. Install Dependencies
Install the required Python packages:

Bash

pip install django langchain langchain-community langchain-huggingface langchain-chroma langchain-ollama sentence-transformers pymupdf
4. Database Setup
Initialize the SQLite database for Django:

Bash

python manage.py migrate
üöÄ Usage
1. Start the Server
Bash

python manage.py runserver
2. Access the App
Open your web browser and go to: https://www.google.com/search?q=http://127.0.0.1:8000/

3. Workflow
Upload: Select one or multiple PDF files. The app will extract text, generate embeddings, and create a summary automatically.

Review: See the auto-generated summary in the file list.

Chat: Type a question in the search bar.

Select "All" to search the whole library.

Select a Specific PDF to narrow down the answer.

üìÇ Project Structure
Ensure your project files are organized as follows for imports to work correctly:

Plaintext

my_project/                   <-- ROOT FOLDER (Open VS Code here)
‚îú‚îÄ‚îÄ manage.py                 <-- Django command tool
‚îú‚îÄ‚îÄ db.sqlite3                <-- Database file (created after migrate)
‚îú‚îÄ‚îÄ chroma_db_data/           <-- (Auto-created folder for vector storage)
‚îÇ
‚îú‚îÄ‚îÄ rag_project/              <-- PROJECT CONFIGURATION FOLDER
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ asgi.py
‚îÇ   ‚îú‚îÄ‚îÄ settings.py           <-- Add 'rag_app' to INSTALLED_APPS here
‚îÇ   ‚îú‚îÄ‚îÄ urls.py               <-- Include 'rag_app.urls' here
‚îÇ   ‚îî‚îÄ‚îÄ wsgi.py
‚îÇ
‚îî‚îÄ‚îÄ rag_app/                  <-- YOUR APP FOLDER
    ‚îú‚îÄ‚îÄ __init__.py           <-- Empty file (Required)
    ‚îú‚îÄ‚îÄ admin.py
    ‚îú‚îÄ‚îÄ apps.py
    ‚îú‚îÄ‚îÄ models.py             <-- Defines UploadedPDF model
    ‚îú‚îÄ‚îÄ urls.py               <-- URL patterns for upload/chat
    ‚îú‚îÄ‚îÄ views.py              <-- Main logic (Upload, Chat, Summarize)
    ‚îú‚îÄ‚îÄ tests.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ templates/            <-- HTML FILES
    ‚îÇ   ‚îî‚îÄ‚îÄ rag_app/
    ‚îÇ       ‚îî‚îÄ‚îÄ upload.html
    ‚îÇ
    ‚îî‚îÄ‚îÄ utils/                <-- HELPER SCRIPTS
        ‚îú‚îÄ‚îÄ __init__.py       <-- Empty file (Required for imports)
        ‚îú‚îÄ‚îÄ embedding.py      <-- HuggingFace Setup
        ‚îú‚îÄ‚îÄ llm.py            <-- ChatOllama Setup
        ‚îú‚îÄ‚îÄ pdf_loader.py     # Extract text from PDF
        ‚îú‚îÄ‚îÄ text_splitter.py  # Split text into chunks
        ‚îî‚îÄ‚îÄ vector_store.py   # ChromaDB logic

‚ùì Troubleshooting
Q: Import "rag_app.utils..." could not be resolved

A: Ensure you have an empty __init__.py file inside the rag_app folder and the rag_app/utils folder.

Q: The answers are too slow.

A: Check that you are using llama3.2 in llm.py. If you are using llama3 (8B parameters), it will be significantly slower on a standard laptop CPU.

Q: sqlite3.OperationalError regarding Chroma.

A: If you are on an older version of Python or Windows, you might need to install pysqlite3-binary. However, standard pip install chromadb usually handles this.
