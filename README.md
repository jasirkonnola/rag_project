# ğŸ“š Django RAG PDF Q&A Project

A simple Retrieval-Augmented Generation (RAG) system built with **Django**, **FAISS**, and **Ollama (LLaMA 3)**.  
It allows users to upload PDFs, automatically indexes their content into a vector store, and then ask natural language questions about the document.

---

## ğŸš€ Features
- **PDF Ingestion**: Upload PDF files via a simple web interface.
- **Text Processing**: Automatically extracts text and splits it into manageable chunks.
- **Vector Storage**: Stores embeddings in a local **FAISS** vector database.
- **AI Answering**: Uses **Ollama (LLaMA 3)** running locally to generate context-aware answers.

---

## ğŸ› ï¸ Tech Stack
- **Django 5.x** â€“ Web framework
- **LangChain** â€“ LLM Orchestration
- **FAISS** â€“ Vector store for similarity search
- **HuggingFace Transformers** â€“ Embedding generation
- **Ollama** â€“ Local LLM runner
- **PyMuPDF** â€“ PDF parsing

---

## âš™ï¸ Installation

### Prerequisites
Ensure you have **Python 3.10+** and **Ollama** installed on your system.

### 1. Clone the repository
```bash
git clone [https://github.com/your-username/rag_project.git](https://github.com/your-username/rag_project.git)
cd rag_project

2. Set up Virtual Environment
Bash

# Create virtual environment named 'tfenv'
python -m venv tfenv

# Activate on Linux/Mac
source tfenv/bin/activate

# Activate on Windows
tfenv\Scripts\activate

3. Install Dependencies
Bash

pip install -r requirements.txt

4. Setup Database
Bash

python manage.py migrate

5. Setup Ollama
Make sure Ollama is installed and pull the LLaMA 3 model:

Bash

ollama pull llama3

6. Run the Server
Bash

python manage.py runserver

ğŸ“‚ Project Structure
rag_project/
â”œâ”€â”€ rag_app/
â”‚   â”œâ”€â”€ templates/rag_app/
â”‚   â”‚   â”œâ”€â”€ base.html
â”‚   â”‚   â””â”€â”€ upload.html
â”‚   â”œâ”€â”€ static/rag_app/css/
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py
â”‚   â”‚   â”œâ”€â”€ text_splitter.py
â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â””â”€â”€ embeddings.py
â”‚   â”œâ”€â”€ views.py
â”‚   â”œâ”€â”€ models.py
â”‚   â””â”€â”€ urls.py
â”œâ”€â”€ rag_project/
â”‚   â”œâ”€â”€ settings.py
â”‚   â”œâ”€â”€ urls.py
â”‚   â””â”€â”€ wsgi.py
â”œâ”€â”€ manage.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ“– Usage
Start Ollama: Open a separate terminal and ensure Ollama is running (ollama serve).

Open Browser: Go to http://127.0.0.1:8000/.

Upload: Use the interface to upload a PDF document.

Ask: Type a question related to the PDF content in the input box.

View Results: The system will retrieve relevant context and generate an answer.

ğŸ“ Notes & Troubleshooting
Ollama Error: If you get a connection error, ensure Ollama is running on port 11434.

FAISS Index: The vector store is saved locally in the faiss_index/ directory. If you want to reset the knowledge base, simply delete this folder and re-upload your PDF.

Production: This project uses DEBUG=True and SQLite. For production, configure .env variables and switch to PostgreSQL.
